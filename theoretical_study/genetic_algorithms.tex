The \gls{qmla} framework lends itself easily to the family of optimsation techniques called \emph{evolutionary algorithms}, 
    where individuals, sampled from a population of candidates, are considered, in generations, as solutions to the given problem,
    and iterative generations aim to efficiently search the available population, 
    by mimicing biological evolutionary mechanisms \cite{back1996evolutionary}. 
In particular, we develop a \gls{es} which incorporates an \gls{ga} in the generation of models;
    \glspl{ga} are a subset of evolutionary algorithms where candidate solutions are expressed as 
    strings of numbers representing some configuration of the system of interest \cite{holland1992adaptation}.
We describe the concepts of \glspl{ga} in \cref{sec:genetic_algorithms}, 
    so we begin by before describing the adaptations which allow us to build a \gls{ges}. 
\par 

\section{Adaptation to QMLA framework}\label{sec:ga_adaptation_to_qmla}
Unlike the generic aspects of \glspl{ga} described in \cref{sec:genetic_algorithms}, 
    in the context of \gls{qmla}, here we must deviate from default mechanisms. 
Recalling the overarching goal of \gls{qmla}, to characterise some black box quantum system, 
    $Q$, we do not have access to a natural \gls{of}.
We wish to optimise the modelling of $\hp$, but assume we do not know the target $\ho$, 
    so we can not simply invoke some loss function, for example. 
Instead, we must devise schemes which exploit the knowledge we \emph{do} have about each candidate $\hj$, 
    which is the primary challenge in building an \gls{es} based on a \gls{ga}.  
We propose and discuss a number of options in \cref{sec:objective_functions}. 
\par 

Common to all proposed \glspl{of}, however, is that candidates should first be trained before evaluation, 
    so that their assessment is based on their actual power in explaining the target system, 
    rather than some initial paramterisation which may not capture their potential. 
This is a tenet of \gls{qmla}: for each candidate $\hj ( \al_j )$, we use a subroutine to optimise $\al_j$, 
    again for this study we rely on \gls{qhl}. 

Ultimately, the conceived role of a \gls{ga} within \gls{qmla} is to generate the sets of models to place 
    on successive branches of the \glspl{et} in \cref{fig:qmla_overview}.
The apparatus for this is to implement an \glsentryfull{es} whose model generation subroutine 
    calls an external \gls{ga}.
Recall from \cref{sec:model_generation}, that we capture the space of available terms as $\termset$, 
    i.e. we list -- in advance -- the feasible terms which may be included in models\footnotemark, 
    with $N_t = | \termset |$ the number of terms considered. 
\gls{qmla} is then an optimisation algorithm, attempting to find the set $\termset^{\prime}$
    which \emph{best} represents the true terms $\termset_0$.
Note, this does not require idenitifcation of the precise true model to be successful, 
    as insight can be gained from approximate models which capture the physics of the target system. 
We introduce metrics for success in \cref{sec:f_score}. 
We recognise the limitations this structure imposes: we can only identify terms which were conceived in advance; 
    this may restrict \gls{qmla}'s applicability to entirely unknown systems, 
    where such a primitive set can not even be compiled. 
\footnotetext{
    Recall that models impose structure on sets of terms: $\hj = \al_j \cdot \vec{T}_j = \sum_{k \in \termset_j} \alpha_k \hat{t}_k$.
}    
\par 

The structure of the overall \gls{qmla} algorithm, recall \cref{fig:qmla_overview},
    is unchanged.
In a \glsentryfull{ges}:
\begin{easylist}[itemize]
    & models are still grouped in branches, here called generations;
    & models are still trained, again through \gls{qhl};
    & branches are evaluated according the the \gls{of} to be described in \cref{sec:objective_functions};
    & new models are spawned through the genetic algorithm by selecting pairs of parents for crossover, 
    with the resultant offspring models probabilistically mutated. 
\end{easylist}


We detail the corresponding \ttt{generate\_models} subroutine in \cref{alg:ga_generate_models}. 
We can restate the informal description of \glspl{ga}, now in the context of \gls{qmla}, as

\begin{easylist}[enumerate]
    \ListProperties(Numbers2=l, Numbers3=r)
    & Sample $N_m$ models from $\population$ at random 
    && this is the first generation, $\mu$. 
    & \label{qmla_ga:loop} Evaluate each model $\hj \in \mu$. 
    && train $\hj$ through \gls{qhl}
    && apply the \glsentrylong{of} to assign the model's fitness $g_j$
    & Map the fitnesses of each model, $\{g_j\}$, to selection probabilities for each model, $\{s_j\}$
    && e.g. by normalising the fitnesses, or by removing some poorly-performing models and then normalising. 
    & Generate the next generation of models
    && Reset $\mu = \{ \}$
    && \label{qmla_ga:select} Select pairs of parents, $\h_{p_1}, \h_{p_2}$, from $\mu$
    &&& Each model's probability of being chosen is given by their $s_j$
    && Cross over $\h_{p_1}, \h_{p_2}$ to produce children models, $\h_{c_1}, \h_{c_2}$. 
    &&& mutate $\h_{c_1}, \h_{c_2}$ according to some random probabilistic process
    &&& keep $\h_{c_i}$ only if it is not already in $\mu$, to ensure $N_m$ unique models are tested at each generation.
    && until $| \mu| = N_m$, iterate to step (\ref{qmla_ga:select}.
    & Until the $N_g^{th}$ generation is reached, iterate to step \ref{qmla_ga:loop}.
    & The strongest model on the final generation is deemed the approximation to the system, $\hp$. 
\end{easylist}



\par 

\subsection{Models as chromosomes}
We first need a mapping from models to chromosomes; 
    this is straightforward given the description of chromosomes as binary strings, 
    exemplified in \cref{sec:knapsack}. 
We assign a gene to every term in $\termset$, so that candidate models are succinctly represented by bit strings of length $N_t$. 
We give an example of the mapping between models and chromosomes in \cref{table:chromosome_example}.

\input{theoretical_study/figures/qmla_chromosome.tex}


\subsection{$F_1$-score}\label{sec:f_score}
We need a metric against which to evaluate models, and indeed the entire \gls{qmla} procedure. 
We can gauge the performance of \gls{qmla}'s model search by the quality of candidate models 
    produced at each generation, so we introduce a mertic to act as proxy for model quality: 
    the $\fs$. 
In short, $f \in \left(0, 1\right)$ indicates the degree to which $\hi$ captures the physics of the target system: 
    $f=0$ indicates that $\hi$ shares no terms with $\ho$, while $f=1$ is found uniquely for $\hi=\ho$. 
We will define the concept formally next. 
Note that here we are able to compute $f$ for candidate models because the target $\ho$
    is simulated, i.e. we know the true terms $\mathcal{T}_0$; 
    this would not be available for a real system with unknown $\ho$, 
    but is useful for the analysis of the algorithm itself. 
\par 

We emphasise that the goal of this work is to identify the \emph{model} which best describes 
    quantum systems, and not to improve on parameter-learning when given access to particular models, 
    since those already exist to a high standard \cite{wiebe2014qhlpra,bairey2019learning}. 
Therefore we can consider \gls{qmla} as a classification algorithm, 
    with the goal of classifying whether individual terms $\hat{t}$ from a set of available 
    terms $\termset = \{\hat{t}\}$ are helpful in describing data which is generated by $\ho$, 
    which has $\termset_0$. 
Candidate models $\hi$ then have $\termset_i$.
We can assess $\hi$ using standard metrics used regularly in the \gls{ml} literature, 
    which simply count the number of terms identified correctly and incorrectly,
\begin{easylist}[itemize]
    & \gls{tp}: number of terms in $\To$ which are in $\Ti$;
    & \gls{tn}: number of terms not in $\To$ which are also not in $\Ti$;
    & \gls{fp}: number of terms in $\Ti$ which are not in $\To$;
    & \gls{fn}: number of terms in $\To$ which are not in $\Ti$.
\end{easylist}
\par 

\noindent These concepts allow us to define 

\begin{easylist}
    & \emph{precision}: how precisely does $\hi$ capture $\ho$,
    i.e. if a term is included in $\Ti$ how likely it is to actually be in $\To$, Eqn \ref{eqn:precision};
    & \emph{sensitivity}: how sensitive is $\hi$ to $\ho$, 
    i.e. if a term is in $\To$, how likely $\Ti$ is to include it, Eqn. \ref{eqn:sensitivity}.
\end{easylist}

\begin{subequations}
    \begin{equation}
        \label{eqn:precision}
        \text{precision} = \frac{TP}{TP + FP}
    \end{equation}
       
    \begin{equation}
        \label{eqn:sensitivity}
        \text{sensitivity} = \frac{TP}{TP + FN}
    \end{equation}
\end{subequations}

Informally, precision prioritises that predicted terms are correct, 
    while sensitivity prioritises that true terms are identified. 
In practice, it is important to balance these considerations. 
$F_{\beta}$-score is a measure which balances these, with $F_1$-score in particular giving them equal importance. 
\begin{equation}
    \label{eqn:f1_score}
    F_1 = \frac{2\times (precision)\times(sensitivity)}{(precision + sensitivity)} = \frac{TP}{TP + \frac{1}{2}(FP + FN)}.
\end{equation}
We give an example of these quantities in \cref{fig:classification_eg}, 
    where $TP=3, TN=4, FP=1, FN=2$, giving $\textmr{precision} = \nicefrac{3}{4}$ 
    and $\textmr{sensitivity} = \nicefrac{3}{5}$, 
    with a final $f = 0.67$, i.e. the average of the indicators of model quality which we care about. 
\par 

We adopt \fs \ as an indication of model quality because we are concerned both with precision and sensitivity
    of output models. 
We can use \fs \ to measure the success of the algorithm,
    by recording $f$ for all models in all generations, allowing us to 
    see whether or not the approximation of the system is improving on average. 

Of course in realistic cases we can not assume knoweldge of $\termset_o$ and therefore cannot compute 
    $\fs$, but it is a useful tool in the development of the \gls{ges} itself, 
    or in cases where $\ho$ is known, such as when the target system is simualated, e.g. in the case of device calibration.
Our search for an effective \gls{of} can then be guided by seeking the 
    method which correlates most strongly with \fs \ in test-cases.

\begin{figure}
    \begin{center}
        \includegraphics[width=\textwidth]{theoretical_study/figures/classication_example.pdf}
    \end{center}
    \caption[Classification concepts]{
        Concepts used for classification. 
        \textbf{a}, the set of available terms $\termset$ containing individual terms $\hat{t}_1$ to $\hat{t}_{10}$. 
        The true model $\ho$ is constructed from the set $\termset_0$. 
        Suppose a candidate $\hp$ has the set $\termset^{\prime}$. 
        \textbf{b}, the confusion matrix for $\hp$. Correctly classified terms are \glsentrylong{tp} and \glsentrylong{tn} (green), 
            and incorrectly classified terms are \glsentrylong{fp} and \glsentrylong{tn} (red). 
    }
    \label{fig:classification_eg}
\end{figure}

\input{theoretical_study/pseudocode_gen_alg_subroutine.tex}

\subsubsection{Distinguishing $\fs$ through \glsentrylongpl{bf}}\label{sec:bf_by_f_score}
We have so far relied on \gls{bf} as the means by which to distinguish models' 
    ability to explain data from the target system. 
We conjecture that models of higher $\fs$ are usually stronger at this task than those of 
    lower $\fs$, which will allow us to incorporate these statistical tools into the design of \glspl{of}. 
We can test this simply by training models of equally spaced $\fs$, and computing \glsentryshort{bf} between all pairs.
\par 

In \cref{fig:bf_bayes}, we show the relationships between $\fs$ and \gls for various conditions: 
Firstly, under a standard training regime with full \gls{bf} comparisons between all pairs, 
    we see that in most cases, the model with higher $\fs$ is favoured by \gls{bf}.
In \cref{fig:bf_by_fscore}b, we run a complete model training subroutine, but compute the \gls{bf} based on fewer experiments and particles 
    (retaining a fraction 0.2).
This verifies an earlier claim from \cref{sec:experiments_for_bf}: 
    although the strength of evidence is weaker given reduced \gls{bf} resources, 
    the direction of the evidence is usually the same, i.e. the insight is indicative of the true physics, 
    so we can save considerable compute time by trusting these restricted \glspl{bf} calculations. 
On the other extreme, we see in \cref{fig:bf_by_fscore}\textbf{c}, where models are trained with, 
    and \glspl{bf} based upon, even greater resources, we see a similar effect: 
    adding resources strengthens the evidence, but does not fundamentally change the outlook. 
Finally, in addition to reducing the resources used per \gls{bf} calculation, 
    we reduce the number of comparisons computed, as would be required for rating models according to \gls{bfeer},
    in \cref{fig:bf_by_fscore}\textbf{d}. 
In essence, we can see that the insight is largely the same 
    from the most and least expensive training/comparison strategies, 
    and by exploiting the available evidence through Elo ratings, rather than 
    brute-force computing as much evidence as possible, we can achieve similar results. 
Note that the time saving reported between full and partial connectivity between models scales with $N_m$: 
    here, with $N_m=10$, the former computes 45 \glspl{bf}, while the latter computes 17;
    for $N_m=60$, as used in full instances/runs, these rise to 600 and 1770 respectively, 
    so the benefit of the Elo scheme is amplified. 
This saving depends on the user's choice of graph connectivity, see \cref{sec:elo_graph}, 
    though is typically a factor between 2-3;
    in general, though, assuming we can reduce the resources used within the \gls{bf}, 
    this phase is considerably less cumbersome than the model training itself, so 
    it is feasible to compute all available \glspl{bf} to inform the \gls{bfeer}.

\begin{figure}
    \includegraphics{theoretical_study/figures/bayes_factors_by_f_scores.pdf}
    \caption[\glsentrylong{bf} by $\fs$.]{
        Pairwise \glsentrylong{bf}, $\bij$ by $\fs$ of candidates $\hi$ ($f_i$ on the $y$-axis) 
        and $\hj$ ($f_j$ on the $x$-axis).
        $\log_{10}\bij > 0 \ (<0)$, green (purple), indicates statistical evidence that $\hi$ ($\hj$) 
        is the better model with respect to the observed data.
        Visualisation is curtailed to $\log_{10} \bij = \pm 50$. 
        \textbf{a}, Models are trained with $\Ne=500, \Np=2500$,
            and all available data uis used in the calculation of \glspl{bf}. 
        \textbf{b}, $\Ne=500, \Np=2500$ using only a fraction (0.2) of experiments/particles for \gls{bf} calculation. 
        \textbf{c}, $\Ne=1000, \Np=5000$, using all available data in the calculation of \gls{bf}. 
        \textbf{d}, $\Ne=500, \Np=2500$, comparing only a subset of pairs of models through \gls{bf}, 
            and using only a fraction ($0.2$) of experiments/particles for those calculations. 
            This pairwise comparison strategy is used for the \gls{bfeer} \gls{of}. 
        \textbf{Inset}, timings for each approach in seconds, with $t=1\textrm{hr}$ marked vertically in blue. 
    }
    \label{fig:bf_by_fscore}
\end{figure}


\subsection{Hyperparameter search}
\begin{figure}
    \begin{center}
        \includegraphics{theoretical_study/figures/gen_alg_param_sweep.pdf}
    \end{center}
    \caption[Genetic algorithm parameter sweep]{
        Genetic algorithm parameter sweep.
        Each subplot shows the success rates for varying numbers of generations, $N_G \in \{8, 16, 32, 64\}$, 
        and numbers of models per generation, $N_m \in \{8, 16, 32, 64\}$. 
        A subplot is generated for ranges of the mutation rate, $r_m$ and the number of 
        generations for which the elite model is unchanged after which the \gls{ga} is cut off. 
    }
    \label{fig:ga_param_sweep}
\end{figure}

Firstly we will validate our reasoning that $\fs$ is a sensible figure of merit, 
    by directly invoking it as the \glsentrylong{of}. 
That is, we first implement a \gls{ga}, using the mapping between models and chromosomes outlined above,
    where we fix the numbers of sites $d=4$, and assume full connectivity between the sites, with $x-$, $y-$ and $z-$ couplings available,
        such that there are $N_t = 3 \times {4 \choose 2} = 18$ terms in $\termset$, so that the total population is of size $2^{18}$ chromosomes.
We can then sweep over the \gls[ga] hyperparameters to find a suitable configuration:
    in \cref{fig:ga_param_sweep} we show how the choice of parameters affect the success rate of preciesly identifying the 
    target chromosome, which is chosen at random for each instance, and we run 20 instance of each configuration. 
The studied hyperparameters\footnotemark \ are
\begin{easylist}[enumerate]
    \ListProperties(Letters=r)
    & number of generations;
    & number of models per generation;
    & mutation rate, $r_m$;
    & number of generation a candidate must reign as the strongest observed, before the search terminates, the \emph{cutoff}. 
\end{easylist}
\footnotetext{  
    These and further hyperparameters can be swept using code given in the \gls{qmla} codebase, 
    in the directory \ttt{scripts/genetic\_alg\_param\_sweep}.  
}
Naturally, we expect that running for more generations with more models per generation will result in a more effective search in the model space, 
    having examined $N_gN_m$ models. 
We must also consider, however, that -- in realistic cases of \gls{qmla} -- the total computation time scales badly with these, 
    since training and comparing models are such expensive subroutines. 
Our goal is therefore to identify the set of hyperparameters which best searches the model space with minmial $N_g, N_m$.  
We see that, unsurprisingly, the \gls{ga} performs poorly when run with few resources, 
    but broadly the performances are similiar provided it is run with sufficient resources.
We can bound the parameters $r_m \geq 0.1, cutoff \geq 5, N_m \geq 16, N_g \geq 16$ to ensure a reasonable 
    search through the model space, without having to consider a prohibitive number of models. 
We must bear in mind, however, that this parameter sweep refers only to the trivial case where 
    the $\fs$ is used as the \gls{of}, so we do not expect such high success rates in realistic cases.

\section{Objective functions}\label{sec:objective_functions}
We have alluded to the central probelm in building a \gls{ga} into \gls{qmla}:
    how to evaluate trained candidate models in the absence of a natural \gls{of}. 
Here we will propose and analyse a number of potential \glspl{of}, 
    some of which will underlie later studies in this thesis. 
Readers may prefer to skip to \cref{sec:obj_fnc_selection},
    where we conclude this study by choosing a single \gls{of} for consideration in this chapter.
\par 
We will show how each \gls{of} computes $g_i$ for candidate models $\hi$, 
    and summarise the outcomes in \cref{table:objective_functions}.
For each $\hi$, we may refer to 
\begin{easylist}[itemize]\label{list:obj_fnc_terms}
    & $\tll_i$, \glsentryfull{tltl},  introduced in \cref{sec:total_log_total_likelihood}
    & $k_i$: the model's cardinality, i.e. number of terms in its parameterisation;
    & $\expset_i$: the bespoke set of experiments composed by the \gls{edh} solely for trainng $\hi$;
    & $n = |\expset_i|$: the number of samples used in trainng $\hi$. 
\end{easylist}
In \cref{table:objective_functions}, 
    we consider six models as examples of the outcome using each \gls{of}; 
    the models, randomly generated of varying quality with respect to the target $\ho$, are

\renewcommand{\arraystretch}{1.25} % space between rows
\setlength{\tabcolsep}{2pt}

\begin{equation}
    \label{eqn:obj_fnc_eg_models}
    \begin{align}
        \hat{H}_0 \ &= \ $\sigma_{(1, 2)}^{z}\sigma_{(1, 3)}^{z}\sigma_{(2, 3)}^{z}\sigma_{(2, 5)}^{z}\sigma_{(3, 5)}^{z}$;\\
        \hat{H}_a \ &= \ $\sigma_{(1, 5)}^{z}\sigma_{(3, 4)}^{z}\sigma_{(4, 5)}^{z}$; \\
        \hat{H}_b \ &= \ $\sigma_{(1, 4)}^{z}\sigma_{(1, 5)}^{z}\sigma_{(2, 5)}^{z}\sigma_{(3, 4)}^{z}$; \\
        \hat{H}_c \ &= \ $\sigma_{(1, 2)}^{z}\sigma_{(1, 5)}^{z}\sigma_{(2, 4)}^{z}\sigma_{(2, 5)}^{z}\sigma_{(4, 5)}^{z}$; \\
        \hat{H}_d \ &= \ $\sigma_{(1, 3)}^{z}\sigma_{(1, 4)}^{z}\sigma_{(1, 5)}^{z}\sigma_{(2, 4)}^{z}\sigma_{(2, 5)}^{z}\sigma_{(3, 4)}^{z}\sigma_{(3, 5)}^{z}$; \\
        \hat{H}_e \ &= \ $\sigma_{(1, 2)}^{z}\sigma_{(1, 3)}^{z}\sigma_{(1, 5)}^{z}\sigma_{(2, 3)}^{z}\sigma_{(2, 5)}^{z}\sigma_{(4, 5)}^{z}$; \\
        \hat{H}_f \ &= \ $\sigma_{(1, 2)}^{z}\sigma_{(1, 3)}^{z}\sigma_{(2, 3)}^{z}\sigma_{(2, 4)}^{z}\sigma_{(2, 5)}^{z}\sigma_{(3, 4)}^{z}\sigma_{(3, 5)}^{z}$.
    \end{align}
\end{equation}

\begin{table}    
    \input{theoretical_study/figures/obj_fnc_table.tex}
    \caption[Objective function examples]{
        Examples of how each objective function, $g$ as described in \cref{sec:inverse_ll} to \cref{sec:elo},
            assign selection probability (denoted \%) to the same set of candidate models, $\{\hi\}$, 
            when attempting to learn data from $\ho$, listed in \cref{eqn:obj_fnc_eg_models}. 
        For each model we first summarise its
            average likelihood $\lk_{e}$ (Eqn. \ref{eqn:log_likelihood}),     
            total log-likelihood $\tll_{i}$ (Eqn. \ref{eqn:log_total_likelihood}), 
            as well as $\fs$ and number of terms $k$.
        We use $n=250$ samples, i.e. $\tll_{i}$ is a sum of $n$ likelihoods.  
        The set of models is truncated so that only the strongest four are assigned selection probability. 
    }
    \label{table:objective_functions}
\end{table}

\subsection{Inverse Log-likelihood}\label{sec:inverse_ll}
$\tll_{i}$ can be thought of as a measure of the success of a given model at explaining data from any set of experiments, $\expset$. 
This can be immediately interpreted as an \gls{of}, provided each candidate model 
    computes a meaningful \gls{tltl}, requiring that they are all based on the same set of experiments, $\expset_v$,
    which are designed explicitly for the purpose of model evaluation. 
\par 

\gls{tltl} are negative and the strongest model has lowest $\left| \tll_{i} \right|$ (or highest $\tll_{i}$ overall),
    so the corresponding \gls{of} for candidate $\hi$ is 
\begin{equation}
    \label{eqn:obj_log_likelihood}
    g_i^{L} = \frac{-1}{\tll_{i}}.
\end{equation}

In our tests, Eqn. \ref{eqn:obj_log_likelihood} is found to be too generous to poor models, 
    assigning them non-negligible probability. 
Its primary flaw, however, is its reliance on $\expset_v$: 
    in order that the \gls{tltl} is significant, it must be based on meaningful experiments, 
    the design of which can not be gauranteed in advance, or at least risks introducing strong bias. 

\subsection{Akaike Information Criterion}\label{sec:akaike_info_criterion}
A common metric in model selection is \gls{aic} \cite{dr2002model}.
Incorporating \gls{tltl}, 
    \gls{aic} objectively quantifies how well a given model accounts for data from the target system,
    and punishes models which use extraneous parameters, 
    by incurring a penalty on $k_i$. 
\gls{aic} is given by 
\begin{equation}
    \label{eqn:aic}
    AIC_i = 2k_i - 2 \tll_{i}.
\end{equation}

In practice we use a slightly modified form of Eqn. \ref{eqn:aic} which
    corrects for the number of samples $n=\left|\expset_i\right|$, 
    called the \gls{aicc}, 
\begin{equation}
    \label{eqn:aicc}
    AICC_i = AIC_i + 2k_i \frac{k_i+1}{n-k_i-1}. 
\end{equation}

Model selection from a set of candidates occurs simply by selecting the model with $AICC_{\textrm{min}}$.
A suggestion to retrieve selection probability,
    by using Eqn. \ref{eqn:aicc} as a measure of \emph{relative likelihood},
    is to compute \emph{Akaike weights} (as defined in in Chapter 2 of \cite{dr2002model}),
\begin{equation}
    \label{eqn:akaike_weights}
    w_i^A = exp \left( \frac{{AICC_{\textrm{min}} - AICC_i}}{2} \right), 
\end{equation}
where $AICC_{\textrm{min}}$ is the lowest $AICC$ observed among the models under consideration
    e.g. all models in a given generation. 

Clearly, Akaike weights impose quite strong penalties 
    on models which do not explain the data well, 
    but also punish models with extra parameters, i.e. overfitting models, 
    effectively searching for the strongest and simplest model simultaneously.
The level of punishment for poorly performing models is likely too drastic: 
    very few models will be in a range sufficiently close to $AICC_{\textrm{min}}$ 
    to receive a meaningful Akaike weight, 
    suppressing diversity in the model population.
    Indeed, we can see from Table \ref{table:objective_functions} that this results in most 
    models being assigned negligible weight, which is not useful for parent selection. 
Instead we compute a straightforward quantity as the \gls{aic}--inspired fitness, Eqn. \ref{eqn:akaike_fitness},
\begin{equation}
    \label{eqn:akaike_fitness}
    g_i^{A} = \left(\frac{1}{AICc_i}\right)^2,
\end{equation}
    where we square the inverse \gls{aic} to amplify the difference in quality between models, 
    such that stronger models are generously rewarded.

\par 

\subsection{Bayesian Information Criterion}\label{sec:bayes_info_criterion}
Related to the idea of \gls{aic}, Eqn. \ref{eqn:aic}, 
    is that of \gls{bic}, 
\begin{equation}
    \label{eqn:bic}
    BIC_i = k_i \ ln(n_i) - 2 \tll_{i},
\end{equation}
    where $k_i, n_i$ and $\tll_i$ are as defined on \cpageref{list:obj_fnc_terms}.
    % where $k, \tll_{i}$ are as defined above and $n$ is the number of samples.
Analagously to Akaike weights, 
    \emph{Bayes weights} as proposed in \S7.7 of \cite{friedman2001elements}, are given by

\begin{equation}
    w_i^B = exp\left( - \frac{BIC_i}{2}  \right).
\end{equation}

\gls{bic} is harsher than \gls{aic} in its punishment of the number of parameters in each model, 
    therefore requiring strong statistical justification for the addition of any parameters. 
Again, this may be overly cumbersome for our use case:
    with such a relatively small number of parameters, 
    the punishment is disproportionate; 
    moreover since we are trying to uncover physical interactions, 
    we do not necessarily want to suppress models merely for 
    their cardinality, since this might result in favouring 
    simple models which do not capture the physics.  
As with Akaike weights, then, we opt instead for a simpler objective function,
\begin{equation}
    \label{eqn:bic_fitness}
    g_i^B = \left( \frac{1}{BIC_i}\right)^2.
\end{equation}
    

\subsection{Bayes factor points}\label{sec:bf_points}
A cornerstone of model selection within \gls{qmla} is the calculation of \glsentryfull{bf} (see \cref{sec:bayes_factors}). 
We can compute the pairwise \gls{bf} between two candidate models, $\bij$, according to Eqn. \ref{eqn:bf_succinct}.
$\bij$ can be based on some evaluation dataset, $\expset_v$, but can also be calculated from $\expset_i \cup \expset_j$: 
    this is a strong advantage since the resulting insight (Eqn. \ref{eqn:bf_cases}) is based on 
    experiments which were bespoke to both $\hi, \hj$. 
As such we can be confident that this insight accurately points us to the stronger of two candidate models. 
\par 

We can utilise this facility by simply computing the \gls{bf} between all 
    pairs of models in a set of $N_m$ candidates $\{\hi\}$, 
    i.e. compute ${N_m \choose 2}$ \gls{bf}s. 
Note that this is computationally expensive: 
    in order to train $\hi$ on $\expset_j$ requires a further $\left| \expset_j \right|$ experiments, 
    each requiring $N_P$ particles\footnote{Caveat the reduction in overhead outlined in \cref{sec:experiments_for_bf}.}, 
    where each particle corresponds to a unitary evolution and therefore the caluclation of a matrix exponential. 
The combinatorial scaling of the model space is then quite a heavy disadvantage. 
However, in the case where all pairwise \gls{bf} are performed, 
    we can assign a point to $\hi$ for every comparison which favours it. 
\begin{equation}
    \label{eqn:bf_points}
    g_i^p = \sum_{j \in \mu} b_{ij}, \ \ \ \ b_{ij} = 
        \begin{cases}
            1, \ \ \ \ \bij > 1 \\
            0, \ \ \ \ \text{otherwise}
        \end{cases}
\end{equation}
This is a straightforward mechanism, but is overly blunt
    because it does not account for the strength of the evidence
    in favour of each model. 
For example, a dominant model will receive only a slightly higher selection probability 
    than the second strongest, even if the difference between them was $\bij = 10^{100}$. 
Further, the unfavourable scaling make this an expensive method. 

\subsection{Ranking}\label{sec:bf_ranking}
Related to \cref{sec:bf_points}, we can rank models in a generation based on their number of \gls{bf} points.
\gls{bf} points are assigned as in Eqn. \ref{eqn:bf_points}, 
    but instead of corresponding directly to fitness, 
    we assign models a rank $R$, 
    i.e. the model with highest $g_i^p$ gets $R=1$, 
    and the model with $n^{th}$ highest $g_i^p$ gets $R=n$. 
Note here we truncate $\mu$, meaning we remove the worse-performing models and retain only $N_m^{\prime}$ models, 
    before calculating $R$.
This is because computing $R$ using all $N_m$ models results in less distinct selection probabilities. 

\begin{equation}
    \label{eqn:ranking}
    g_i^R = \frac{N_m^{\prime}-R_i+1}{\sum\limits_{n=1}^{N^{\prime}_m} n},
\end{equation}
    where $R_i$ is the rank of $\hi$ and $N_m^{\prime}$ is the number of models retained after truncation. 


\subsection{Residuals}\label{sec:residuals}
Recall at each experiment, $N_P$ particles are compared against a single experimental datum, $d$. 
For consistency with QInfer \cite{qinfer-1_0} -- on which \gls{qmla}'s code base extends --
    we call the expectation value for the system $\Pr_Q(0)$, 
    and that of each particle $\Pr_p(0)$, recall \cref{sec:likelihood}. 
Typically, $\Pr_Q(0) = \left| \bra{\psi} e^{-i\ho t} \ket{\psi} \right|^2$, 
    but this can be changed to match given experimental schemes, 
    e.g. the Hahn-echo sequence applied in \cite{gentile2020learning}. 
By definition, the datum $d$ is the binary outcome of the measurement on the system under experimental conditions $e$.
That is, $d$ encodes the answer to the question: 
    after time $t$ under Hamiltonian evolution, did $Q$ project onto 
    the basis we have labelled $0$ (usually the same as the input probe state $\ket{\psi}$)?
However, in practice we often have access also to the \gls{likelihood} , 
    i.e. rather than a binary value, a number representing the probability that 
    $Q$ will project on to $0$ for a given experiment $e$, $\Pr_Q(0 | e)$. 
Likewise, we can simulate this quantity for each particle, $\Pr_p(0 | e)$. 
This allows us to calculate the \emph{residual} between the system and individual particles' likelihoods, 
    $r_p^e$, as well as the mean residual across all particles in a single experiment $r^e$:
\begin{align}
    \label{eqn:particle_residual}
    \begin{split}
    r_p^e & = \left| Pr_Q(0 | e) - Pr_p(0 | e) \right|  \\
    r^e &= \underset{p}{\text{mean}} \{r_p^e\}
    \end{split}
\end{align}
\par 

Residuals capture how closely the particle distribution reproduced the dynamics from $Q$:
    $r^{e}_{p} = 0$ indicates perfect preditiction, while $r^e_p=1$ is completely incorrect. 
We can therefore maximise the quantity $1-r$ to find    the best model, 
    using the \gls{of}
\begin{equation}
    \label{eqn:residual_fitness}
    g^r_i = \left| 1 - \underset{e \in \expset}{\text{mean}}\{ {r^e} \}\right|^2.
\end{equation}
\par     
This \gls{of} can be thought of in frequentist terms 
    as similar to the residual sum of squares,
    although instead of summing the residual squares, we average to ensure $0 \leq r \leq 1$. 
$g_i^r$ encapsulates how well a candidate model can reproduce dynamics 
    from the target system, as a proxy for whether that candidate describes the system. 
This is not always a safe figure of merit: 
    in most cases, we do not expect parameter learning to perfectly optimise $\al_i$. 
Reproduced dynamics alone can not capture the likelihood that $\hi=\ho$. 
However, this \gls{of} provides a useful test for \gls{qmla}'s \gls{ga}:
    by simulating the case where parameters \emph{are} learned perfectly, 
    such that we know that $g_i^r$ truly represents the ability of $\hi$ to 
    simulate $\ho$, then this \gls{of} gaurantees to promote  the strongest models,
    especially given that $\hi=\ho \implies r_p^e=0 \ \forall \ \{e, p\}$. 
In realistic cases, however, the non-zero residuals -- even for 
    strong $\hi$ -- may arise from imperfectly learned parameters,
    rendering the usefulness of this \gls{of} uncertain. 
Finally, it does not account for the cardinality, $k_i$, of the candidate models, 
    which could result in favouring severely overfitting models in order to gain marginal improvement 
    in residuals, which all machine learning protocols aim to avoid in general.

\subsection{Bayes factor enhanced Elo-ratings}\label{sec:elo}
A popular tool for rating individual competitors in sports and games is the \emph{Elo rating} scheme, 
    e.g. used to rate chess players and soccer teams \cite{elo1978rating, fifa_elo}, 
    also finding application in the study of animal hierarchies \cite{neumann2011assessing}. 
Elo ratings allow for evaluating relative quality of individuals 
    based on incomplete pairwise competitions, 
    e.g. despite two football teams having never played against each other before, 
    it is possible to quantify the difference in quality between those teams, 
    and therefore to predict a result in advance \cite{hvattum2010using}. 
We recognise a parallel with these types of competitions by noting that
    in our case, we similarly have a pool of individuals (models), 
    which we can place in direct competition, and quantify the comparitive outcome through \gls{bf}. 

\par 
Elo ratings are transitive: given some interconnectivity in a generation, 
    we need not compare \emph{every} pair of models in order to 
    make meaningful claims about which are strongest;
    it is sufficient to perform a subset of comparisons, 
    ensuring each individual is tested robustly. 
We can take advantage of this transitivity to reduce the combinatorial overhead 
    usually associated with computing bespoke \gls{bf} between all models 
    (i.e. using their own training data $\expset_i$ instead of a generic 
    $\expset_v$).
In practice, we map models within a generation to nodes on a graph,
    which is then sparsely connected. 
In composing the list of edges for this graph, 
    we primarily prioritise each node having a similar number of edges,
    and secondarily the distance between any two nodes. 
For example, with 14 nodes we overlay edges such that each node 
    is connected with 5,6 or 7 other nodes, 
    and all nodes at least share a competitor in common. 
\par 

The Elo rating scheme is as follows: 
    upon creation, $\hi$ is assigned a rating $R_i$; 
    every comparison with a competitor $\hj$ results in $\bij$; 
    $R_i$ is updated according to the known strength of its competitor, $R_j$, 
    as well as the result $\bij$. 
The Elo update ensures that winning models are rewarded 
    for defeating another model, 
    but that the extent of that reward reflects the quality of its opponent. 
As such, this is a fairer mechanism than \gls{bf} points, 
    which award a point for every victory irrespective of the opposition:
    if $\hj$ is already known to be a strong or poor model, 
    then $\Delta R_i$ proportionally changes the credence we assign to $\hi$. 
It achieves this by first computing the \emph{expected} result of a given comparison
    with respect to each model, based on the current ratings, 

\begin{subequations}
    \label{eqn:elo_expected_score}
    \begin{equation}
        E_i 
        = \frac{1}{1 + 10^{\frac{R_j - R_i}{400}}} ;
    \end{equation}
    \begin{equation}
        E_i + E_j = 1,
    \end{equation}    
\end{subequations}

Then, we find the binary \emph{score} from the perspective of each model:
\begin{equation}
    \label{eqn:elo_score}
    \begin{cases}
        B_{ij} > 1 & \Rightarrow \ S_i = 1; \ S_j =0  \\
        B_{ij} < 1 & \Rightarrow \ S_i = 0; \ S_j = 1 \\
    \end{cases}
\end{equation}
which is used to determine the change to each model's rating:
\begin{equation}
    \label{eqn:elo_delta_r}
    \Delta R_i = \eta \times \left( S_i - E_i \right).
\end{equation}

An important detail is the choice of $\eta$, i.e. the \emph{weight} of the 
    change to the models' ratings. 
In standard Elo schemes this is a fixed constant, 
    but here -- taking inspiration from football ratings where $\eta$ is the number of goals 
    by which one team won -- we weight the change by the strength of our belief in the outcome: 
    $\eta \propto \left| \bij \right|$.
That is, similarly to the interpretation of Eqn. \ref{eqn:bf_cases}, 
    we use the evidence in favour of the winning model to transfer points from the loser to the winner,
    albeit we temper this by instead using $\eta = log_{10}(B_{ij})$, since \gls{bf} can give very large numbers. 
In total, then, following the comparison between models $\hi, \hj$, we can perform the Elo rating update
\begin{equation}
    \label{eqn:elo_update}
    R_i^{\prime} = R_i + \text{log}_{10}(B_{ij}) \left(S_i - \frac{1}{1 + 10^{\frac{R_j - R_i}{400}}}\right).
\end{equation}
This procedure is easiest to understand by following the example in Table \ref{table:elo_eg}. 

\begin{table}
    \centering
    \input{theoretical_study/figures/elo_example.tex}
    \caption[Example of Elo rating updates.]{
        Example of Elo rating updates. 
        We have two models, where $\h_a$ is initially believed to be a stronger candidate than $\h_b$, 
            i.e. has a higher starting Elo rating.     
        We show the effect of strong evidence\footnotemark \ in favour of each model following \gls{bf} comparison, $\bij \sim 10^{100}$. 
        In the case where $\h_a$ defeats $\h_b$, because this was so strongly expected given their initial ratings, 
            the reward for $\h_a$ (and cost to $\h_b$) is relatively small, 
            compared with the case where -- contrary to prediction -- $\h_b$ defeats $\h_a$. 
    }
    \label{table:elo_eg}
\end{table}
\footnotetext{
    Note to achieve $\bij = 10^{100} = e^{\tll_{i} - \tll_{j}} \implies \tll_{i} - \tll_{j} = ln(10^{100}) \approx 7$.
}


\par 
Finally, it remains to select the starting rating $R_i^0$ to assign models upon creation. 
Although this choice is arbitrary, it can have a strong effect on the progression of the algorithm. 
Here we impose details specific to the \gls{qmla} \gls{ga}: 
    at each generation we admit the top two  models automatically 
    for consideration in the next generation, 
    such that strongest models can stay alive in the population and ultimately win. 
These are called \emph{elite} models, $\hat{H}_e^1, \hat{H}_e^2$. 
This poses the strong possibility for a form of generational wealth:
    if elite models have already existed for several generations, 
    their Elo ratings will be higher than all alternatives by defintion. 
Therefore by maintaining a constant $R_i^0$, 
    i.e. a model born at generation 12 gets the same $R_i^0$ as $\hat{H}_e^1$ -- which was 
    born several generations prior and has been winning \gls{bf} comparisons ever since --  
    we bias the \gls{ga} to continue to favour the elite models. 
Instead, we would prefer that newly born models can overtake the Elo rating of elite models. 
We achieve this through an imprecise mechanism:
    newly born models are given the Elo rating of the second-most-elite model, $\h_e^2$. 
This performs three key functions: 
\begin{easylist}[enumerate]
    \ListProperties(Numbers=r, Start=0)
    & new models are immediately \emph{within range} of the elite models; 
    if they perform well enough, they have a realistic and fair chance of overtaking them; 
    & the strongest model retains some of its advantage gained over previous generations 
    -- in order that the ratings are meaningful, there must be some advantage accrued over series of victories; 
    & $\hat{H}_e^2$ is allowed continue to compete, but has no advantage:
    in order to retain its status as elite, it must perform well \emph{again} in this generation,
    so it can not simply rely on results from previous generations -- against inferior opposition -- 
    to remain active in the gene pool. 
    
\end{easylist}

\par 

Given the arbitrary scaling of the Elo rating scheme, 
    and in order to derive a meaningful selection probability, 
    we ought to ground the raw Elo rating somehow at each generation $\mu$. 
We do this by subtracting the lowest rating among the entertained models, $R_{\textrm{min}}^{\mu}$.
This serves to ensure the range of remaining $R_i$ is defined only by the difference between
    models as assessed within $\mu$: 
    a very strong model might have much higher $R_i$ than its contemporaries, 
    but that difference was earned exclusively by comparison within $\mu$, 
    so it deserves higher fitness. 
We perform this step before truncation, so that the reamining models post-truncation
    all have non-zero fitness. 
Finally, then, we name this \gls{of} the \emph{Bayes-factor enhanced Elo rating}, 
    whereby the fitness of each model is attained directly from its rating
    after undergoing Elo updates in the current generation minus the minimum rating of any model 
    in the same generation $\mu$,

\begin{equation}
    \label{eqn:elo_fitness}
    g_i^E = R_i^{\mu} - R_{\textrm{min}}^{\mu}.
\end{equation}
The advantage of this \gls{of} is that it gives a meaningful value on the absolute quality of every model, 
    allowing us to determine the strongest, and importantly to find the relative strength between models. 
Further, it exploits bespoke \glspl{bf}, i.e. based on the considered models' 
    individually designed $\expset_i$,
    removing the impetus to design $\expset_v$ which can
    evaluate models definitively. 
One disadvantage is that it does not explicitly punish models based 
    on their cardinality, 
    however this feature is partially embedded by adopting \gls{bf} for the comparisons, 
    which are known to protect against overfitting.

\subsection{Choice of objective function}\label{sec:obj_fnc_selection}
\begin{figure}
    \centering
    \includegraphics[width=0.95\textwidth]{theoretical_study/figures/objective_fnc_comparison.pdf}
    \caption[Comparison between proposed \glsentrylongpl{of}.]{
        Comparison between proposed \glsentryfullpl{of}. 
        Each \gls{of} trains the same initial generation of $N_m=28$ models with resources
        $N_E=500, N_P=2000$, and then design a new set of $N_m$ models through 
        the same roulette strategy, such that the only difference between \gls{of}'s output 
        is how they assign selection probability.
        We run each \gls{of} 25 times for the same target system, 
            a $4$--qubit Heisenberg--XYZ model. 
        \textbf{(a)} shows the box--plot of new models' \fs, \ $f$, 
            where the median and inter--quartile ranges are indicated by the boxes,
            as well as those of the initial generation $\mu$ centered on $f_{\mu}=0.45$.
            We mark $f=\{0.4, 0.5, ..., 1.0\}$ for ease of interpretation. 
        \textbf{(b)} shows box--plots of the time taken to compute the single generation in each case.
        In \textbf{(c)} we report the difference between the median $f$ among the 
            newly proposed models from $f_{\mu}$, $\Delta f$,
            plotted against the time to achieve the result. 
    }
    \label{fig:obj_fnc_comparison}
\end{figure}

Having proposed a series of possible objective functions, 
    we are now in a position to analyse which of those are most appropriate for \gls{qmla}. 
Recall from \cref{sec:f_score} the figure of merit we use for models, \fs, 
    which we will use to distinguish between the outputs of each \gls{of}.
\par 

First we can remark on the examples listed in \cref{table:objective_functions}. 
The \glspl{of} which rely on the \gls{tltl}, i.e. $g^L, g^A, g^B, g^r$, 
    are effectively tricked by the log likelihood, which appears reasonably convincing for 
    poor models, e.g. $\h_a, \h_c$. 
This underlines the risk in buidling $\expset_v$, which can be biased towards weak models, 
    for example resulting in high selection probability for $\h_a$ which has $f=0$, 
    while $\h_d$, with $f=0.4$ is discarded. 
On the other hand, \glspl{of} grounded by the \gls{bf} ($g^p, g^R, g^E$) invariably 
    promote models of higher $\fs$, justifying the role of statistical evidence 
    used for those calculations. 
Overall, however, the insights from this complete example are insufficient to 
    make general claims about the performance of each \gls{of}, 
    so here we examine their outputs systematically. 
\par 

Returning to the task of determining our favoured \gls{of}, 
    we choose some random target $\ho$, 
    and run a single generation using each \gls{of}, 
    judging them by the quality of models they produce.
We train the same batch of $N_m=28$ random models in each case, and allow each \gls{of} 
    to compute the selection probabilities for those models, 
    and therefore direct the design of the hypothetical next generation of models. 
We plot the distribution of \fs \ that each \gls{of} produces in Fig. \ref{fig:obj_fnc_comparison},
    also accounting for the time taken in each case, i.e. 
    we report the time to train and evaluate the single generation on a 16--core node.
\par

Overall, then, we can see that a strong balance of outcome with resource considerations 
    are achieved by the Bayes factor enhanced Elo rating strategy, \cref{sec:elo}
    so we use that for the case study presented in this chapter. 
We strongly emphasise, however, that the performance of each objective function
    can vary under alternative conditions, and therefore similar analysis may 
    be warranted for future applications. 
For instance, if $t_{max}$ is known to be small, 
    in smaller model spaces, using $g^r$ results in higher success rates.
We retain \gls{bfeer}, however, for generality and novelty, 
    but it is important to recognise that the results listed do not reflect
    an upper limit of \gls{qmla}'s performance, 
    but rather reflect the constraints of the system under study; 
    each $Q$ will bring its own unique considerations which can result in 
    significantly stronger or weaker performance. 
In particular, we will later use the \gls{of} based on residuals, \cref{sec:residuals}, 
    to study a much larger model space under assumptions of perfect parameter learning, \cref{chapter:many_qubits}.



\section{Application}
Having introduced all the necessary concepts of \glspl{ga}, 
    mapped them to the \gls{qmla} framework and chosen a suitable 
    \gls{of}, we can finally use the \gls{ges} for model search. 
In summary of this chapter so far, we use the following settings. 
\begin{easylist}[itemize]
    & Models are mapped to a bit string (chromosome), where each bit represents whether a given
        model term (gene) is present; chromosomes are of length $N_t$ genes. 
    & A maximum of $N_g$ generations are run, each with $N_m$ unique models. 
    & Candidate models are trained using \gls{qhl}, specifically by using \gls{iqle}\footnotemark 
        for parameter estimation. 
    & Models' fitness are determined by their \gls{bfeer}, 
        after having been trained by \gls{qhl} 
        and compared against some set of competing candidate models. 
    & For generating models on $\mu+1$, the models on $\mu$ are first truncated 
        (the worst-performing $\nicefrac{N_m}{2}$ are discarded), and then assigned 
        selection probability based on their fitness. 
    & Models are selected to become parents sequentially using roulette selection. 
        Highly favoured models can parent many children models. 
    & Selected parent models are crossed over via a one point cross-over,
        at crossover location $\kappa \in \left( \frac{N_t}{4}, \frac{3N_t}{4} \right)$, 
        and probabilistically mutated with rate $r_m=0.25$. 
    & The top two elite models from $\mu$ are included on the $\mu+1$.
    & If, after 5 generations, the highest-fitness (elite) model is unchanged, 
        we terminate the search and declare that model as the champion, $\hp$. 
    & Otherwise, after $N_g$ generations, the highest-fitness model on the final generation is 
        declared $\hp$.
\end{easylist}
\footnotetext{
    \gls{iqle} assumes complete access to the target system see \cref{sec:iqle}. 
    This restricts the present analysis to simulateable, rather than physical, use cases, 
    e.g. device calibration. 
}

We will use a four-qubit model space under the Heisenberg formalism, \cref{eqn:heisenberg_model_general}, 
    such that any pair of sites $\bkl$ can be coupled by any of $\s^x_{\bkl}, \s^y_{\bkl}m \s^z_{\bkl}$, 
    so in total there are $N_t = |\termset| = 3 \times {4 \choose 2} = 18$ terms, 
    giving a model space of $2^{18} \approx 250,000$ viable models/chromosomes. 
For practical reasons\footnotemark, we set $N_m=60$ and $N_g=16$, although in most cases the 
    elitism clause is triggered so the search terminates long before $N_g$ is reached. 
\footnotetext{
    This is to ensure, with 15 available worker nodes, and accounting for some slowly-learning models, 
    that all $N_m$ models in a generation are trained within $2 t_{qhl}$, 
    where $t_{qhl}$ is the time to train a single model. 
}
The true parameters $\al_0$ are assigned randomly in the range $(0.25, 0.75)$; 
    within \gls{qhl} the prior is set as a multivariate normal distribution $0.5 \pm 0.125$. 
We choose $\ho$ at random to contain half the available terms\footnotemark,
\begin{equation}
    \label{eqn:gen_alg_true_model}
    \ho = \sigma_{(1, 2)}^{yz}\sigma_{(1, 3)}^{z}\sigma_{(1, 4)}^{y}\sigma_{(2, 3)}^{xy}\sigma_{(2, 4)}^{x}\sigma_{(3, 4)}^{xz}.
\end{equation} 
\footnotetext{
    Note we use a compact model representation, e.g. 
    $\hi = \sigma_{(1, 2)}^{yz}\sigma_{(1, 3)}^{z} = \sigma_{(1, 2)}^{y} + \sigma_{(1, 2)}^{z} + \sigma_{(1, 3)}^{z}$.
}
\par 
\subsection{Analysis}
We will analyse the \gls{ges} from four perspectives: 
    a single model, a single generation, a single \gls{qmla} \gls{instance}, 
    and the overall performance across many instances, i.e. a \gls{run}.
\par 

\begin{figure}
    \begin{center}
        \includegraphics{theoretical_study/figures/single_generation_ratings_progression.pdf}
    \end{center}
    \caption[Single model within a single generation of the \gls{qmla} \glsentrylong{ga}.]{
        \glsentrylong{bfeer} progression for a single candidate, $\hi$, within a single generation.
        \textbf{a}, The \glspl{bf} between $\hi$ and some opponents, $\{\hj\}$, 
            from the perspective where $\hi$ \emph{wins} given $\bij > 1 \Rightarrow \log_{10}\bij > 0$, 
            and loses otherwise. 
        \textbf{b}, $\hi$'s rating is shown (solid green line) changing according to the \glspl{bf} 
            comparisons with 12 other models from the same generation. 
            Before each comparison, $\hi$'s rating is shown (green cross)
            as well as the rating of its opponent, $\hj$ (purple plus).
            The $F_1$-scores are also shown for $\hi$ (dashed green line) and $\hj$ (purple diamond).
        \textbf{c}, The corresponding change in $\hi$'s rating, $\Delta R_i$. 
    }
    \label{fig:single_models_elo_ratings}
\end{figure}

Recall that \gls{bfeer} are mediated through random graphs: 
    given $N_m$ models on $\mu$, a given model $\hi$ undergoes some 
    $N^i_{BF} < N_m$ \gls{bf} comparisons. 
In \cref{fig:single_models_elo_ratings} we show the \gls{bf} results 
    and effects on the rating of a random model, $\hi$, where $N_m=60$ and $N^i_{BF}=12$,
    i.e. $\hi$ is directly compared against $\sim20\%$ of contemporary models on $\mu$. 
We see that $\hi$'s rating is effected by whether it wins a given comparison, 
    but also by the strength of evidence provided by the comparison (the \gls{bf}), 
    and the quality of its opposition (its rating).


\par 
\begin{figure}
    \begin{center}
        \includegraphics{theoretical_study/figures/single_generation_all_ratings.pdf}
    \end{center}
    \caption[Ratings of all models in a single \glsentrylong{ga} generation.]{
        Ratings of all models in a single \glsentrylong{ga} generation.
        Each line represents a unique model and is coloured by the $\fs$ of that model. 
        \textbf{Inset}, the selection probabilities resulting from the final ratings of this generation. 
        Only a fraction of models are assigned selection probability, while the remaining poorer-performing 
        models are truncated. 
    }
    \label{fig:single_generation_all_ratings}
\end{figure}

We extend the single model analysis of \cref{fig:single_models_elo_ratings} to all $N_m$ models 
    in the first generation in \cref{fig:single_generation_all_ratings}.
The general trend is that models of higher $\fs$ have their ratings increased, 
    at the expense of models of lower $\fs$. 
After assessing models thus, the set of models is truncated to retain only the strongest candidates,
    which are assigned probability of being chosen to become a parent during roulette selection, 
    as in \cref{sec:reproduction}.
The very strongest two models are granted a position in the subsequent generation: 
    these are the \emph{elite} models. 

\par 
\begin{figure}
    \begin{center}
        \includegraphics{theoretical_study/figures/gen_alg_instance_combined.pdf}
    \end{center}
    \caption[\Gls{instance} of \gls{qmla} \glsentrylong{ga}.]{
        A single \gls{instance} of the \gls{qmla} \glsentrylong{}{ga}.
        \textbf{a}, Ratings of all models for the first five generations. 
        Each line in each generation represents a model by its $\fs$. 
        Horizontal dotted lines show the starting rating at that generation. 
        \textbf{b}, Gene pool progression for $N_m=60, N_g=15$. 
        Each time at each generation represents a model by its $\fs$. 
    }
    \label{fig:ga_instance}
\end{figure}

Similarly we can consider the quality of models across at each generation,
    and their ratings. 
In \cref{fig:ga_instance} we see the trend suggested by \cref{fig:single_generation_all_ratings} continue,
    i.e. that models of higher quality overall tend to achieve higher \gls{bfeer}.
The gene pool as a whole tends towards a homogeneous set of models, all with $f \geq 0.85$. 
Consequently, even in cases where the precise model, $\ho$, is not identified, the champion model
    is highly informative, in that it captures many of the same interactions, 
    therefore most-likely providing meaningul insight on the system's physics. 

\par 
\begin{figure}
    \begin{center}
        \includegraphics{theoretical_study/figures/gen_alg_run.pdf}
    \end{center}
    \caption[\Gls{run} of \gls{qmla} \glsentrylong{ga}.]{
        A \gls{run} of the \gls{qmla} \glsentryfull{ga}, consisting of 100 independent \glspl{instance}.
        \textbf{(a)}, The model space contains $2^{18}\approx250,000$ candidate models; 
            normally distributed around $f=0.5 \pm 0.14$. 
        \textbf{(b)}, The models explored during the model search of all instances combined, 
            $\{\hat{H}_i\}$, show that \gls{qmla} tends towards stronger models overall, 
            with $f = 0.76 \pm 0.15$ from $\sim~43,000$ chromosomes across the instances, 
            i.e. each instance trains $\sim~430$ distinct models. 
        \textbf{(c)}, Champion models from each instance, showing \gls{qmla} finds strong models 
            in general, and in particular finds the true model $\ho$ (with $f=1$) in $72\%$ of cases, 
            and $f \geq 0.88$ in all instances. 
        \textbf{(d)}, Hinton diagram showing the rate at which each term is found in the winning model. 
            The size of the blocks show the frequency with which they are found, while the colour indicates 
            whether that term was in the true model (blue) or not (red).
            Terms are couplings between two qubits, e.g $\hat{\sigma}_{(1, 3)}^{x}$ 
                couples the first and third qubits along the $x$-axis. 
            We test four qubits with full connectivity, resulting in 18 unique terms 
            (terms with black rectangles are not considered by the \gls{ga}).
}
    \label{fig:ga_run}
\end{figure}

Finally, to understand the performance of the \gls{qmla} algorithm overall, 
    we run 100 independent instances in a \gls{run}, \cref{fig:ga_run}. 
We see that, while the overall model space can be characterised by the distribution 
    of models' $\fs$, where we sample where $\overbar{f} = 0.5 \pm 0.14$, 
    that \gls{qmla} quickly moves to the subspace of high-quality models, 
    i.e. the models explored have median $f = 0.76 \pm 0.15$.
This exploration is based on $430 \pm 45$ chromosomes per instance, 
    i.e. \gls{qmla} trains only $0.16\%$ of the $2^{18}$ potential models. 
Ultimately \gls{qmla} nominates champion models, $\{ \hp \}$ with $f \geq 0.88$ in all instances, 
    and precisely identifies $\hp=\ho$ in $72\%$ of instances. 
Considering the big picture, where the remit of \gls{qmla} is to identify the interactions 
    the target system is subject to, we show how often each term/gene is included in $\hp$ in 
    \cref{fig:ga_run}\textbf{d}. 
Crucially, we see that terms which really are within the true Hamiltonian, $\hat{t} \in \termset_0$, 
    are found significantly more frequently than those without, $\hat{t} \notin \termset_0$. 
This level of analysis can be used to post-validate the outcome of \gls{qmla}, 
    i.e. rather than relying on $\hp$ from a single instance, 
    trusting the terms' individual frequencies as evidence that they are of importance when describing 
    the system of interest. 
Of particular 

\subsection{Device characterisation}
This adaptive \gls{ges} may prove a useful application of \gls{qmla} in the domain of device calibration,
    in particular to characterise some untrusted quantum simulator.
That is, by using the simulator to implement some target $\ho$, 
    \gls{qmla} can identify which operator is \emph{actually} implemented.
For instance, implementation of a four--qubit model
    relies on high-fidelity two-qubit gates between arbitrary qubit pairs, 
    and \gls{qmla} can effectively reconstruct which operations were and were not faithfully computed.
