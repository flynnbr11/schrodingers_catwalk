\glsresetall

A \emph{model} is the mathematical description of a quantum system of interest, \gls{q}.
In \cref{chapter:qhl}, we discussed a number of systems in terms of their Hamiltonian descriptions,
    although in general the description of a quantum system need not be Hamiltonian, 
    e.g. Lindbladian models describe open quantum systems, 
    so we will generically refer to the \emph{\gls{model}} of \gls{q} throughout. 
\par 

\gls{qmla} is an algorithm that extends the concept of applying machine learning to the 
    characterisation of Hamiltonians we've seen in \cref{chapter:qhl}. 
The extension, and central question of \gls{qmla} is:
    if we do not know the structure of the model which describes a target quantum system, 
    can we still learn about the physics of the system?
That is, we remove the assumption about the form of the Hamiltonian model, 
    and attempt to uncover which \emph{\glspl{term}} constitute the Hamiltonian, 
    and in so doing, learn the interactions the system is subject to. 
\par 

For the remainder of this thesis, our objective is to learn the model underlying 
    a series of target quantum systems.
We will first introduce some concepts which will prove useful when discussing \gls{qmla}, 
    before describing the protocol in detail in \cref{sec:qmla_protocol}.


% %%%%%%%%%%% MODELS %%%%%%%%%%% 
\section{Models}\label{sec:models}
\Glspl{model} are simply the mathematical objects which can be used to predict the behaviour of a system. 
In this thesis, \glspl{model} are synonymous with Hamiltonians,
    composed of a set of \emph{\glspl{term}}, $\termset = \{ \hat{t} \}$, 
    where each $\hat{t}$ is a matrix. 
Each term is assosiated with a multiplicative scalar, which may be referred to as that term's \emph{parameter}: 
    we impose order on the terms and parameters such that we can succinctly summarise any model as 

\begin{align}
    \label{eqn:model}
    \begin{split}
        \hat{H} = \irow{\alpha_0 & \dots & \alpha_n} \icol{\hat{t}_1 \\ \vdots \\ \hat{t}_n} = \al \cdot \terms 
    \end{split}
\end{align}
    where $\al, \terms$ are the model's parameters and terms, respectively.

For example, a model which is the sum of the (non-identity) Pauli operators is given by
\begin{align}
    \label{eqn:pauli_sum_model}
    \begin{split}
        \hat{H} &= \irow{\alpha_x & \alpha_y & \alpha_z} \cdot \icol{\sx \\ \sy \\ \sz} \\
        &= \alpha_x \sx + \alpha_y \sy + \alpha_z \sz \\
        &= \begin{pmatrix}
            \alpha_z & \alpha_x - i \alpha_y \\
            \alpha_x + i\alpha_y & \alpha_z
        \end{pmatrix}.
    \end{split}
\end{align}
\par 

Through this formalism, we can say that the sole task of \gls{qhl} was to optimise $\al$, given $\terms$. 
The principle task of \gls{qmla} is to identify the terms $\terms$ 
    which are supported by the most statistical evidence as describing the target system \gls{q}. 
In short, \gls{qmla} proposes \emph{candidate models} $\hi$ as hypotheses to explain \gls{q}; 
    we \emph{train} each model independently through a parameter learning routine, 
    and finally nominate the model with the best performance after training. 
In particular, \gls{qmla} uses \gls{qhl} as the parameter learning \emph{subroutine}, 
    but in principle this step can be performed by any algorithm which learns $\al$ for given $\terms$, 
    \cite{wang2015hamiltonian, krastanov2019stochastic, flurin2020using, niu2019learning, 
    greplova2017quantum, lokhov2018optimal, acampora2019evolutionary, burgarth2017evolution, valenti2021scalable}. 
While discussing a model $\hi$, their \emph{training} then simply means the implementation 
    of \gls{qhl}\footnote{Or the chosen parameter learning subroutine.}, 
    where $\hi$ is \emph{assumed} to represent \gls{q}, 
    such that $\al_i$ is optimised as well as it can be, 
    even if $\hi$ is entirely inaccurate. 

% %%%%%%%%%%% BAYES FACTORS %%%%%%%%%%% 
\section{Bayes factors}\label{sec:bayes_factors}
We can use the tools introduced in \cref{sec:total_log_total_likelihood} to \emph{compare} candidate models. 
Of course it is first necessary to ensure that each model has  
    been adequately trained: while inaccurate models are unlikely to strongly 
    capture the system dynamics, they should first train on the system 
    to determine their best attempt at doing so, 
    i.e. they should undergo the process in \cref{chapter:qhl}.
It is statistically meaningful to compare models via their \gls{tltl}, $\tll_i$, 
    if and only if they have considered the same data, 
    i.e. if models have each attempted to account for the same set of experiments, $\expset$ \cite{kass1995bayes}.
\par 

We can then exploit direct pairwise comparisons between models,  
    by imposing that both models' \gls{tltl} are computed based 
    on \emph{any} shared set of experiments $\expset$, 
    with corresponding measurements $\expdata = \{ d_{e}\}_{e \in \expset}$.
Pairwise comparisons can then be quantified by the \gls{bf},
\begin{equation}
    \label{eqn:bayes_factors}
    B_{ij} = \frac{\Pr(\expdata | \hi; \expset)}{\Pr(\expdata | \hj; \expset)}.
\end{equation}
Intuitively, we see that the \gls{bf} is the ratio of the likelihood, i.e. the performance, 
    of model $\hi$'s attempt to account for the data set $\expdata$ observed following the 
    experiment set $\expset$, against the same \gls{likelihood} for model $\hj$.
\glspl{bf} are known to be statistically signicative of the stronger model 
    from a pair, at explaining observed data,
    while favouring models of low cardinality, thereby supressing overfitting models. 
\par 

We have that, for independent experiments, and recalling \cref{eqn:total_likelihood}, 
\begin{align}
    \begin{split}
        \Pr(\expdata | \hi; \expset) &= \Pr(d_n | \hi; e_n) \times \Pr(d_{n-1} | \hi; e_{n-1}) \times \dots \times \Pr(d_0 | \hi; e_0) \\
        &= \prod_{e \in \expset} \Pr(d_e | \hi; e) \\
        &= \prod_{e \in \expset} (\lk_{e})_i.
    \end{split}
\end{align}

We also have, from \cref{eqn:log_total_likelihood}
\begin{align}
    \label{eqn:log_likelihood}
    \begin{split}
        \tll_{i} &= \sum_{e \in \expset} ln\bk{\bk{\lk_{e}}_i} \\
        \implies e^{\tll_{i}} &= \exp \bk{ \sum_{e \in \expset} ln \left[ \bk{\lk_{e}}_i\right] } 
        = \prod_{e \in \expset} \exp \bk{ ln \left[ (\lk_{e})_i \right]  } 
        = \prod_{e \in \expset} (\lk_{e})_i.
    \end{split}
\end{align}

So we can write 
\begin{align}
    \label{eqn:bf_by_ll}
    \begin{split}
        \bij &= \frac{\Pr(\expdata | \hi; \expset)}{\Pr(\expdata | \hj; \expset)} 
        = \frac{ \prod\limits_{e \in \expset} (\lk_{e})_i } {\prod\limits_{e \in \expset} (\lk_{e})_j } 
        = \frac{e^{\tll_{i}}}{e^{\tll_{j}}} \\
    \end{split}
\end{align}

\begin{equation}
    \label{eqn:bf_succinct}
    \implies \bij = e^{\tll_{i} - \tll_{j}}    
\end{equation}


This is simply the exponential of the difference between two models' total log-likelihoods when presented the same set of experiments. 
Intuitively, if $\hi$ performs well, and therefore has a high \gls{tltl}, $\tll_{i}=-10$, 
    and $\hj$ performs worse with $\tll_{j}=-100$, then $B_{ij} = e^{-10-(-100)} = e^{90} \gg 1$.
Conversely for $\tll_{i}=-100, \tll_{j}=-10$, then $B_{ij} = e^{-90} \ll 1$. 
Therfore $\lvert B_{ij} \rvert$ is the strength of the statistical evidence
    in favour of the interpretation 

\begin{equation}
    \label{eqn:bf_cases}
    \begin{cases}
        B_{ij} > 1 & \Rightarrow \hi \text{\ favoured over \ } \hj \\
        B_{ij} < 1 & \Rightarrow \hj \text{\ favoured than \ } \hi \\
        B_{ij} = 1 & \Rightarrow \hi \text{\ equally favoured as \ } \hj. \\
    \end{cases}
\end{equation} 

\subsection{Experiment sets}\label{sec:experiments_for_bf}
As mentioned it is necessary for the \gls{tltl} of both models in a \gls{bf} calculation to
    refer to the same set of experiments, $\expset$. 
There are a number of ways to achieve this, 
    which we briefly summarise here for reference later. 
\par 

During training (the \gls{qhl} subroutine), candidate model $\hi$ is trained against $\expset_i$, 
    designed by an \gls{edh} to optimise parameter learning specifically for $\hi$;
    likewise $\hj$ is trained on $\expset_j$. 
The simplest method to compute the \gls{bf} is to enforce $\expset=\expset_i \cup \expset_j$ 
    in \cref{eqn:bayes_factors}, i.e. to cross-train $\hi$ using the data designed specifically for training $\hj$, 
    and vice versa. 
This is a valid approach because it challenges each model to attempt to explain experiments
    designed explicitly for its competitor,   
    at which only truly accurate models are likely to succeed. 
\par 
A second approach builds on the first, but incorporates \emph{burn--in} time in the training regime:
    this is a standard technique in the evaluation of \gls{ml} models whereby its earliest iterations 
    are discounted for evaluation so as not to skew its metrics, 
    ensuring the evaluation reflects the strength of the model. 
In \gls{bf}, we achieve this by basing the \gls{tltl} only on a subset of the training experiments. 
For example, the latter half of experiments designed during the training of $\hi$, $\expset_i^{\prime}$. 
This does not result in less predictive \gls{bf}, since we are merely removing the 
    noisy segments of the training for each model, e.g. the first half of experiments in \cref{fig:param_learning_vary_particles}. 
Moreover it provides a benefit in reducing the computation requirements: 
    updating each model to ensure the \gls{tltl} is based on $\expset^{\prime}=\expset_i^{\prime} \cup \expset_j^{\prime}$
    requires only half the computation time, 
    which can be further reduced by lowering the number of particles used during the update, $\Np^{\prime}$, 
    which will give a similar result as using $\Np$, assuming the posterior has converged.
We will verify this claim later in \cref{sec:bf_by_f_score}, in the context of real examples.
\par 

A final option is to design a set of \emph{evaluation} experiments, $\expset_v$, 
    that are valid for a broad variety of models, and so will not favour any particular model.
Again, this is a common technique in \gls{ml}: to use one set of data for training models, 
    and a second, unseen dataset for evaluation. 
This is clearly a favourable approach: 
    provided for each model we compute \cref{eqn:log_total_likelihood} using $\expset_v$,
    we can automatically select the strongest model based solely on their \glspl{tltl}, 
    meaning we do not have to perform further computationally-expensive updates, 
    as required to cross-train on opponents' 
    experiments during \gls{bf} calculation. 
However, it does impose on the user to design a \emph{fair} $\expset_v$, 
    requiring unbiased \gls{probe} states $\{\ket{\psi}\}$ and times $\{t\}$ on a timescale 
    which is meaningful to the system under consideration. 
For example, experiments with $t > T_2$, the decoherence time of the system, 
    would result in measurements which offer little information, 
    and hence it would be difficult to extract evidence in favour of any 
    model from experiments in this domain.
It is difficult to know, or even estimate, such meaningful time scales a priori,
    so it is difficult for a user to design $\expset_v$. 
Additionally, the training regime each model undergoes during \gls{qhl}
    is designed to provide adaptive experiments that take into account
    the specific model entertained, to choose an optimal set of evolution times, 
    so it is likely that the set of times in $\expset_i$ is \emph{reasonable} by default. 
This approach would be favoured in principle, in the case where such constraints can be accounted for,
    e.g. an experiment repeated in a laboratory where the available 
    \gls{probe} states are limited and the timescale achievable is understood. 
     
% %%%%%%%%%%% PROTOCOL %%%%%%%%%%%     
\section{Quantum Model Learning Agent Protocol}\label{sec:qmla_protocol}
Given a target quantum system, \gls{q}, described by some \emph{true} Hamiltonian model, $\ho$, 
    \gls{qmla} distills a model $\hp \approx \ho$.
We can think of \gls{qmla} as a forest search algorithm\footnotemark:
    consisting of a number of trees, each of which can have an arbitrary number of branches, 
    where each leaf on each branch is an individual model, 
    \gls{qmla} is the search for the leaf in the forest with the 
    strongest statistical evidence of representing \gls{q}. 
\footnotetext{
    Note \gls{qmla} is not a random forest, where decision trees are added at random, 
    because in \gls{qmla} trees are highly structured and included manually. 
}    
Each tree in the \gls{qmla} forest corresponds to an independent \emph{ \gls{model search} }, 
    structured according to a bespoke \gls{es}, which we detail in \cref{sec:exploration_strategies}. 
\par 
In short, the components of the iterative \gls{model search} for a given \gls{es}, 
    depicted in \cref{fig:qmla_overview}\textbf{(a-d)}, are
\begin{description}
    \item[Branches] A set of candidate models, $\{ \hi \}$, are held together on a branch, $\mu$. 
    \item[Training] Each model $\hi \in \mu$ is trained according to a parameter learning subroutine.
    \item[Consolidation] The performance of candidates in $\mu$ are ranked relative to each other, 
        such that some models are favoured over others, for instance through selection of a \emph{branch champion}, $\hat{H}_C^{\mu}$. 
        Consolidation can rely on any statistical test, with \glspl{bf} providing a robust platform to disinguish 
        any pair of candidates.
    \item[Spawn] A set of new models are constructed, accounting for the immediately prior consolidation stage, 
        i.e. leveraging the best-yet-known models to construct similar hypotheses.
\end{description}

Following the iterative model generation procedure, the model selects the strongest considered candidate,
    for instance by consolidating the set of branch champions, $\{\hat{H}_C^{\mu} \}$,
    resulting in the nomination of a single \emph{champion model}, $\hat{H}_{S}^{\prime}$, \cref{fig:qmla_overview}\textbf{(e)}.
Multiple model searches can proceed in parallel, and they are each assigned an independent \gls{et}, $S$. 
The final step of \gls{qmla} is then to consolidate the set of champion models from all \glspl{et}, $\{\hat{H}_{S}^{\prime} \}$, 
    in order to delcare a \emph{global} \gls{champion model}, $\hp$, \cref{fig:qmla_overview}\textbf{(f)}. 

\begin{figure}[t!]
    \begin{center}
        \includegraphics{algorithms/figures/overview.jpg}
    \end{center}
    \caption[Quantum Model Learning Agent overview]{
        Schematic of \acrfull{qmla}. 
        \textbf{(a-d)} \Gls{model search} phase within an \Glsentryfull{es}.
        \textbf{(a)} Models are placed as (empty, purple) nodes on the \emph{active branch} $\mu$, 
            where each model is a sum of terms $\hat{t}_k$ multiplied by corrresponding scalar parameters $\alpha_k$. 
        \textbf{(b)} Each active model is trained according to a subroutine such as 
            quantum Hamiltonian learning to optimise $\al_i$, 
            resulting in the trained $\hat{H}(\al_i^{\prime})$ (filled purple node). 
        \textbf{(c)} $\mu$ is consolidated, i.e. models are evaluated relative to other
            models on $\mu$, according to the consolidation mechanism specificed by the \gls{es}.
            In this example, pairwise Bayes factors, $B_{ij}$, between $\hi, \hj$ are computed, 
            resulting in the election of a single branch champion $\hat{H}_C^{\mu}$ (bronze). 
        \textbf{(d)} A new set of models are \emph{spawned} according to the chosen
            \gls{es}'s model generation strategy.
            In this example, models are spawned from a single parent. 
            The newly spawned models are placed on the next layer, $\mu+1$, 
            iterating back to \textbf{(a)}.
        \textbf{(e-f)} Higher level of entire QMLA procedure.
        \textbf{(e)} The  \gls{model search}  phase for a unique \gls{es} is presented on an \emph{exploration tree}. 
            Multiple \gls{es} can operate in parallel, e.g. assuming different underlying physics, 
            so the overall \gls{qmla} procedure involves a \emph{forest search} across multiple exploration trees.
            Each \gls{es} nominates a champion, $\hat{H}_{S}^{\prime}$ (silver), 
            after consolidating its branch champions (bronze). 
        \textbf{(f)} $\hat{H}_{S}^{\prime}$ from each of the above exploration trees are gathered on a single layer, 
            which is consolidated to give the final \gls{champion model}, $\hp$ (gold). 
    }
    \label{fig:qmla_overview}
\end{figure}

% %%%%%%%%%%% EXPLORATION STRATEGIES %%%%%%%%%%% 
\section{Exploration Strategies}\label{sec:exploration_strategies}
\gls{qmla} is implemented by running $\Nt$ \glspl{et} concurrently, 
    where each \gls{et} corresponds to  a unique  \gls{model search}  and ultimately nominates a single 
    model as its favoured approximation of $\ho$. 
An \gls{es} is the set of rules which guide a single \gls{et} throughout its  \gls{model search} . 
We elucidate the responsibilities of \glspl{es} in the remainder of this section, but in short they can be summarised as: 

\begin{easylist}[enumerate]
    \ListProperties(Numbers=r)
    & model generation: 
        combining the knowledge progressively acquired on the \gls{et} to construct new candidate models;
    & decision criteria for the  \gls{model search}  phase:
        instructions for how \gls{qmla} should respond at predefined junctions, 
        e.g. whether to cease the  \gls{model search}  after a branch has completed;
    & \gls{true model} specification:
        detailing the terms and parameters which constitute $\ho$ (in the case where \gls{q} is simulated);
    & modular functionality: 
        subroutines called throughout \gls{qmla} are interchangeable such that each \gls{es} specifies the 
        set of functions to achieve its goals.
\end{easylist}
\par 

\gls{qmla} acts in tandem with one or more \glspl{es}, through the process depicted in \cref{fig:qmla_flow}. 
In summary: 
    \gls{qmla} sends a request to the \gls{es} for a set of models; 
    \gls{es} designs models and places them as leaves on a new branch on its \gls{et}, and returns the set $\mathbb{H}$; 
    \gls{qmla} places $\mathbb{H}$ on a unique layer; 
    \gls{qmla} trains the models in $\mathbb{H}$; 
    \gls{qmla} consolidates $\mathbb{H}$;
    \gls{qmla} informs the \gls{es} of the results of training/consolidation of $\mathbb{H}$; 
    \gls{es} decides whether to continue the search, and informs \gls{qmla}.
\input{algorithms/figures/qmla_flow.tex}

\subsection{Model generation}\label{sec:model_generation}
The main role of any \gls{es} is to design candidate models to test against $\ho$. 
This can be done through any means deemed appropriate, 
    although in general it is sensible to exploit the information gleaned so far in the \gls{et}, 
    such as the performance of previous candidates and their comparisons, 
    so that successfel models are seen to \emph{\gls{spawn}} new models, 
    e.g. by combining previously successful models, or by building upon them. 
Conversely, model generation can be completely determined in advance or entirely random.
This alludes to the central design choice in composing an \gls{es}: 
    how broad and deep should the searchable \emph{model space} be, 
    considering that adequately training each model
    is expensive, and that model comparisons are similarly expensive. 
The  \gls{model search}  occurs within some \emph{\gls{model space}}, the size of which can usually be easily found 
    by assuming that terms are binary -- either the interaction they represent is present or not. 
If all possible terms are accounted for, and the total set of terms is $\termset$,
    then there are $2^{\absval{\termset}}$ available candidates in the model space. 
The \gls{model space} encompasses the closed\footnotemark \ set of models construable by the set of terms considered by an \gls{es}. 
Because training models is slow in general,
    a central aim of \gls{qmla} is to search this space efficiently,
    i.e. to minimise the number of models considered, while retaining high quality models and 
    providing a reasonable prospect of uncovering the \gls{true model}, or a strong approximation thereof. 

\footnotetext{
    It is feasible to define an \gls{es} which uses an open model space, that is, there is no pre-defined $\termset$, 
    but rather the \gls{es} determines models through some other heuristic mechanism. 
    In this thesis, we do not propose any such \gls{es}, but note that the \gls{qmla} framework 
    facilitates the concept, see \cref{chapter:sw}.   
}



\subsection{Decision criteria for the model search phase}
Further control parameters, which direct the growth of the \gls{et}, are set within the \gls{es}.
At several junctions within \cref{alg:qmla}, \cref{alg:model_search}, 
    \gls{qmla} queries the \gls{es} in order to decide what happens next.
Here we list the important cases of this behaviour. 

\begin{description}

    \item[Parameter-learning settings] \
    
    \begin{easylist}[itemize]
    && such as the prior distribution to assign each parameter during \gls{qhl}, and the parameters needed to run \gls{smc}.
    && the time scale on which to examine \gls{q}.
    && the input probes to train upon, $\Psi$, described in \cref{sec:probes}. 
    \end{easylist}
    
    \item[Branch comparison strategy] \
    \begin{easylist}
    && How to consolidate models within a branch. Some examples used in this work are
        &&& a points-ranking, where all candidates are compared via \gls{bf} and points are assigned to the favoured model according to \cref{eqn:bf_cases};
        &&& ranking reflecting each model's log-likelihood (\cref{eqn:log_likelihood}) after training;
        &&& models are ranked according to some \glsentrylong{of}, as in the case of \glspl{ga} which we detail in \cref{chapter:ga}.
    \end{easylist}

    \item[\gls{model search} termination criteria] \
    \begin{easylist}    
    && e.g. instruction to stop after a fixed number of iterations, or when a certain fitness has been reached.        
    \end{easylist}
    
    \item[Champion nomination] \
    \begin{easylist}    
    && when a single \gls{et} is explored, identify a single champion from the branch champions, $\{\huc\}$;
    && if multiple \glspl{et} are explored, the mechanism to compare champions across trees. 
    \end{easylist}
\end{description}

\subsection{True model specification}
It is necessary also to specify details about the \gls{true model}, $\ho$, 
    at least in the case where \gls{qmla} acts on simulated data. 
Within the \gls{es}, we can set $\terms_0$ as well as $\al_0$. 
For example where the target system is an untrusted quantum simulator to be characterised, 
    $S_u$, by interfacing with a trusted (quantum) simulator $S_t$, 
    we decide some $\ho$ in advance:
    the model training subroutine calls for  \glspl{likelihood}, 
    those corresponding to $\ho$ are computed $S_u$, 
    while particles'  \gls{likelihood} are computed on $S_t$. 

\subsection{Modular functionality}\label{sec:modular_functionality}
Finally, there are a number of fundamenetal subroutines which are called upon throughout the \gls{qmla} algorithm. 
These are written independently such that each subroutine has a number of available implementations. 
These can be chosen to match the requirements of the user, and are set via the \gls{es}. 

\begin{description}
    \item[Model training procedure] \
    \begin{easylist}
        && i.e. whether to use \gls{qhl} or quantum process tomography, etc. 
        && In this work we always used \gls{qhl}.     
    \end{easylist}

    \item[ \gls{likelihood} function] the method used to estimate the \gls{likelihood} 
        for use during \gls{qle} within \gls{qhl}, 
        which ultimately depends on the measurement scheme. 
    \begin{easylist}
        && The role of these functions is to compute the probability of measuring each experimental outcome. 
        % &&& We typically refer to the quantity $\Pr(0)$, which is the probability of measuring \gls{q} in a chosen basis,
        %     which is used for the parameter distribution updates within QInfer \cite{qinfer-1_0}. 
        && These functions compute the \emph{expectation value}
            of the unitary operator, $e^{-i\hat{H} t}$, corresponding to the dynamics of either \gls{q} or the hypothesis model.
        && By default, here we use projective measurement back onto the input \gls{probe} state, 
            $\lvert \bra{\psi} e^{-i\hat{H}t} \ket{\psi} \rvert^2$.        
        && In the usual case where \gls{q} has binary outcomes, we label one outcome 
            -- say, measurement in the state $\ket{+}$--  as $d=0$ and compute $\Pr(0)$
            so that the \gls{likelihood}, expectation value and $\Pr(0)$ 
            refer to the same quantity, see \cref{sec:likelihood}.
        && It is possible instead to implement any measurement procedure, 
            for example an experimental procedure where the environment is traced out,
            as we address in \cref{chapter:nv}.
    \end{easylist}
            
    \item[\gls{probe}] defining the input probes to be used during training, $\Psi$, see \cref{sec:probes}. 
    \begin{easylist}   
        && In general it is preferable to use numerous probes in order to avoid biasing particular terms. 
        && In some cases we are restricted to a small number available input probes, e.g. to match experimental constraints.
    \end{easylist}

    \item[Experiment design heuristic] bespoke experiments to maximise the information 
        on which models are individually trained, described in \cref{sec:heuristic}.
    \begin{easylist}
        && In particular, in this work the experimental controls consist solely of $\{ \ket{\psi}, t \}$. 
        && Currently, probes are generated offline,
            but in principle it is feasible to choose optimal probes based on available or hypothetical information. 
            For example, probes can be chosen as a normalised sum of the candidate model's eigenvectors.
        && Choice of $t$ has a large effect on how well the model can train. 
            By default, times are chosen proportional to the inverse of the 
            current uncertainty in $\al$ to maximise Fischer information, 
            through the multi-particle guess heuristic described in \cref{sec:pgh} \cite{Wiebe:2014qhl}.
        &&& Alternatively, evolution times may be chosen from a fixed set in order to force \gls{qhl} to 
            reproduce the dynamics within those times' scale. 
            For instance, if a small amount of experimental data is available offline, 
            it is sensible to train all candidate models against the entire dataset.  
    \end{easylist}

    \item[Model training prior] specify the structure of the prior distribution, e.g. Fig. \cref{fig:qhl_smc}(a)
    \begin{easylist}
        && Set the initial mean and width of each parameter separately to define the prior multi-dimensional $\Pr(\al)$.
    \end{easylist} 

\end{description}

\subsection{Exploration strategy examples}
To solidifiy the concept of \glspl{es}, and how they affect the overall
    reach and runtime of a given \gls{et}, consider the following examples, 
    where each strategy specifies how models are generated, as well as how trained models are compared within a branch. 
Recall that all of these strategies rely on \gls{qhl} as the model training strategy, 
    so that the run time for training, is $t_{\textrm{QHL}} \sim \Ne\Np t_{U(n)}$, 
    where $t_U(n)$ is the time to compute the unitary evolution via the matrix exponential for an $n$-qubit model 
All models are trained using the default  \gls{likelihood} in \cref{eqn:likelihood}. 
Assume the conditions
\begin{easylist}[itemize]
    & all models considered are represented by 4-qubit models;
    && $t_{U(4)} \sim 10^{-3} \textrm{sec}$. 
    & each model undergoes a reasonable training regime;
    && $\Ne=1000, \Np=3000$;
    && $\implies t_{\textrm{QHL}} = \Ne \times \Np \times  t_{U(4)} = 3000s \sim 1 \textrm{h} $;
    & Bayes factor calculations use 
    && $\Ne=500, \Np=3000 $
    && $\implies t_{\textrm{BF}} \sim 2 \times  500 \times 3000 \times  10^{-3} \sim 1 \textrm{h}$;
    & there are 12 available terms
    && allowing any combination of terms, this admits a \gls{model space} of size $2^{12} = 4096$
    & access to 16 computer cores to parallelise calculations over
    && i.e. we can train 16 models or perform 16 \gls{bf} comparisons in $1\textrm{h}$.
\end{easylist}
\par 

\noindent Then, consider the following model generation/comparison strategies.
\begin{easylist}[enumerate]
    \ListProperties(Numbers1=l, Numbers2=r)
    & \label{gr:predefined} Predefined set of $16$ models, comparing every pair of models
    && Training takes $1\textrm{h}$, and there are ${16 \choose 2} = 120$ comparisons spread across 16 processes, requiring $8\textrm{h}$
    && total time is $9\textrm{h}$. 
    & \label{gr:generative_full} Generative procedure for model design, comparing every pair of models,
        running for 12 branches
    && One branch takes $9\textrm{h} \implies$ total time is $12 \times 9 = 108\textrm{h}$; 
    && total number of models considered is $16 \times 12 = 192$. 
    & \label{gr:generative_sparse} Generative procedure for model design, where less model comparisons are needed 
        (say one third of all model pairs are compared),
        running for 12 branches
    && Training time is still $1\textrm{h}$
    && One third of comparisons, i.e. $40$ \gls{bf} to compute, requires $3\textrm{h}$
    && One branch takes $4 h \implies$ total time is $36 \textrm{h}$
    && total number of models considered is also $192$. 
\end{easylist}

These examples illustrate some of the design decisions involved in \gls{es}s, 
    namely whether timing considerations are more important than thoroughly exploring the model space.
They also show considerable time--savings in cases where it is
    acceptable to forego all model comparisons. 
The approch in (a) is clearly limited in its applicability, 
    mainly in that there is a heavy requirement for prior knowledge, 
    and it is only useful in cases where we either know $\ho \in \mathbb{H}$, 
    or would be satisfied with approximating $\ho$ as the closest available $\hj \in \mathbb{H}$. 
On the opposite end of this spectrum, (c) is an excellent approach
    with respect to minimising prior knowledge required by the algorithm, 
    although at the significant expense of testing a much larger number of candidate models. 
There is no optimal strategy for all use--cases: 
    specific quantum systems of study demand particular considerations, 
    and the amount of prior information available informs how wide the model search should reach. 

\par 

% %%%%%%%%%%% GENERALITY %%%%%%%%%%% 

\section{Generality}
Several aspects of \gls{qmla} are deliberately vague in order to facilitate generality. 

\begin{description}
    \item[\Gls{model}] can mean any description of a quantum system which captures the interactions it is subject to. 
    \begin{easylist}[itemize]
        && Here we exclusively consider Hamiltonian models, but Lindbladian models can also be considered as generators of quantum dynamics. 
    \end{easylist}
    \item[Model training] is any subroutine which can train a given model, i.e. optimise a given parameterisation 
        under the assumption that it represents the target system. 
    \begin{easylist}[itemize]
        && Currently only \gls{qhl} has been implemented, although for example tomography is valid in principle, 
            with its own advantages and disadvantages.
            Overall \gls{qhl} is found to fulfil the remit of model training with a balance of efficiency and rigour \cite{gentile2020learning}.
        && \Gls{qhl} relies on the calculation of a characteristic  \gls{likelihood} function; 
            this too is not restricted to the generic form of \cref{eqn:likelihood} and can be replaced by 
            any form which represents the \gls{likelihood} that experimental conditions $e$ result in measurement datum $d$. 
            We will see examples of this in \cref{chapter:nv} where we trace out part of the system in order 
            to represent open systems. 
    \end{easylist}
    \item[Model selection] or \emph{consolidation} can be as rigourous as desired by the user. 
    \begin{easylist}[itemize]
    && Consolidation occurs at the branch level of each \gls{et}, but also in finding the tree champion, 
        and ultimately the global champion. 
    && In practice, we use either \gls{bf} or a related concept such as \gls{tltl} which are statistically signicative. 
        However, in \cref{chapter:ga} we will consider a number of alternative schemes for discerning the strongest models. 
    \end{easylist}
\end{description}

\subsection{Agency}\label{sec:agency}
While the concept of \emph{agency} is contentious \cite{franklin1996agent}, 
    we can view our overall protocol as a multi-agent system \cite{wooldridge2009introduction}, 
    or even an agent based evolutionary algorithm \cite{sarker2010agent}, 
    because any given \gls{es} satisfies the definition,
    \emph{the  population  of  individuals can be considered as a population of agents}, 
    where we mean the population of models present on a given \gls{et}. 
More precisely, we can view individual models as \emph{learning agents} according to the criteria of 
    \cite{russell2002artificial}, i.e. that a learning agent has
    \begin{easylist}[itemize]
        & a \emph{problem generator}: designs actions in an attempt to learn about the system -- this is precisely the role of the \gls{edh};
        & a \emph{performance element}: implements the the designed actions and measures the outcome
            -- the measurement of a datum following the experiment chosen by the \gls{edh}; 
        & a \emph{critic}: the likleihood function informs whether the designed action (experiment) was successful; 
        & a \emph{learning element}: the updates to the weights and overall parameter distribution improve the model's performance over time. 
    \end{easylist}
We depict this analogy in \cref{fig:learning_agent}.
Finally, the model design strategy encoded in the \gls{es} \emph{can} allow agency,
    by permitting the spawn rules autonomy, 
    so we label the entire procedure as the quantum model learning agent. 

\input{algorithms/figures/agents.tex}


%%%%%%%%%%% PSEUDOCODE %%%%%%%%%%% 
\section{Algorithms}
We conclude this chapter by listing the algorithms used most frequently, 
    in order to clarify each of their roles, and how they interact. 
\cref{alg:qmla} shows the overall \gls{qmla} algorithm, 
    which is simplified greatly to a loop over the  \gls{model search}  of each \gls{es}. 
The  \gls{model search}  iteself is listed in \cref{alg:model_search},
    which contains calls to subroutines for model learning (\gls{qhl}, \cref{alg:qhl}), 
    branch evaluation (which can be based upon \gls{bf}, \cref{alg:bayes_factor})
    and centers on the generation of new models, an example of which -- based on a \emph{greedy search} prerogative -- 
    is given in  \cref{alg:generate_models}. 

\begin{algorithm}
    \caption{Quantum Model Learning Agent}
    \label{alg:qmla}
    \DontPrintSemicolon
    \KwIn{ $Q$ \tcp*[1]{some physically measurable or simulateable quantum system}}
    \KwIn{ $\mathbb{S}$ \tcp*[1]{set of exploration strategies}}\;

    \KwOut{$\hat{H}^{\prime}$ \tcp*[1]{\gls{champion model}}}\;
    
    $\mathbb{H}_c \gets \{\}$\;   
    \For{$S \in \mathbb{S}$ }{
        $\hat{H}_{S}^{\prime} \gets$ \ttt{model\_search(Q, S)} \tcp*[1]{Run model search for this ES}

        $\mathbb{H}_c \gets \mathbb{H}_c \cup  \{\hat{H}_{S}^{\prime}\}$ \tcp*[1]{add ES champion to collection}
    }
    $\hat{H}^{\prime} \gets $ \ttt{final\_champion($\mathbb{H}_c$)}\;
    return $\hat{H}^{\prime}$

\end{algorithm}

\begin{algorithm}
    \caption{ES subroutine: \ttt{model\_search}}
    \label{alg:model_search}
    \DontPrintSemicolon
    \KwIn{ $Q$ \tcp*[1]{some physically measurable or simulateable quantum system}}
    \KwIn{ $S$ \tcp*[1]{Exploration strategy: collection of rules/subroutines}}\;

    \KwOut{$\hat{H}_{S}^{\prime}$ \tcp*[1]{Exploration strategy's nominated \gls{champion model}}}\;
     
    $\nu \gets \{\}$

    $\mathbb{H}_c \gets \{\}$\;   
    \While{ \ttt{!S.terminate()} }{
        $\mu \gets $ \ttt{S.generate\_models($\nu$)} \tcp*[1]{e.g. \cref{alg:generate_models}}\; 

        \For{ $\hi \in \mu$}{
            $\hi^{\prime} \gets $ \ttt{S.train($\hi$)} \tcp*[1]{e.g. \cref{alg:qhl}}
        }
        $\nu \gets$ \ttt{S.consolidate($\mu$)} \tcp*[1]{e.g. pairwise via \cref{alg:consolidate}}

        $\hat{H}_{c}^{\mu} \gets $ \ttt{S.branch\_champion($\nu$)} \tcp*[1]{use $\nu$ to select a branch champion}

        $\mathbb{H}_c \gets \mathbb{H}_c \cup \{\hat{H}_c^{\mu}\}$ \tcp*[1]{add branch champion to collection}
    }

    $\hat{H}_{S}^{\prime} \gets $ \ttt{S.nominate\_champion($\mathbb{H}_c$)}\;
    return $\hat{H}_{S}^{\prime}$

\end{algorithm}

\begin{algorithm}
    \caption{ES subroutine: \ttt{consolidate} (example: points per Bayes factor win)}
    \label{alg:consolidate}
    \DontPrintSemicolon
    \KwIn{ $\mu$ \tcp*[1]{information about models considered to date}}
    \KwIn{ $b$ \tcp*[1]{threshold for sufficient evidence that one model is stronger}}
    \KwIn{\ttt{BF()} \tcp*[1]{callable function to compute the Bayes factor between $\hat{H}_j$ and $\hat{H}_k$, \cref{alg:bayes_factor} }}    
    \KwOut{ $\nu$ \tcp*[1]{information about models' relative performance}}\;
     
    $\mathbb{H} \gets$ \ttt{extract\_models}$(\mu)$

    \For{
        $\hat{H}_j \in \mathbb{H}$
    }{
        $s_j = 0$ \tcp*[1]{Initialise score for each model} 
    }
     
    \For{
        $\hat{H}_j, \hat{H}_k \in \mathbb{H}$
        \tcp*[1]{pairwise Bayes factor between all models in the set}
    }{
        $B \gets \ttt{BF}( \hat{H}_j, \hat{H}_k)$ \;
        \uIf{$B > b$ \tcp*[1]{Increase score of winning model}}{
            $s_j \gets s_j+1$
        }  
        \uElseIf{$B < 1/b $}
        {
            $s_k \gets s_k+1$
        }
    }
    
    $ k^{\prime} \gets \max_k \{s_k\}$ \tcp*[1]{Find which model has most points} 
    
    
    return $\hat{H}_{k^{\prime}}$


\end{algorithm}



\begin{algorithm}
    \caption{ES subroutine: \ttt{generate\_models} (example: greedy spawn)}
    \label{alg:generate_models}
    \DontPrintSemicolon
    \KwIn{ $\nu$ \tcp*[1]{information about models considered to date}}
    \KwIn{ $\mathcal{T}$ \tcp*[1]{set of terms to search}}
    \KwOut{ $\mathbb{H}$  \tcp*[1]{set of candidate models}}\;

    $\huc \gets$ \ttt{top\_model}$(\nu)$
    
    $\mathbb{H} \gets \{\}$ 

    \For{ $\hat{t} \in \mathcal{T}$}{
        $\hat{H}_i \gets \huc + \hat{t}$\;
        $\mathbb{H} \gets \mathbb{H} \cup \{ \hat{H}_i\}$
    }
    
    return $\mathbb{H}$ 

\end{algorithm}



\begin{algorithm}
    \caption{Quantum Hamiltonian Learning}
    \label{alg:qhl}
    \DontPrintSemicolon
    \KwIn{ $Q$ \tcp*[1]{some physically measurable or simulatable quantum system, described by $\ho$}}

    \KwIn{$\hat{H}_i$ \tcp*[1]{Hamiltonian model attempting to reproduce data from $\hat{H}_0$ }}
    \KwIn{$\pra$ \tcp*[1]{probability distribution for $\al = \al_0 $}}
    \KwIn{$\Ne$ \tcp*[1]{number of experiments to iterate learning procedure for}}
    \KwIn{$\Np$ \tcp*[1]{number of particles to draw from $\pra$}}
    \KwIn{\ttt{$\Lambda(\pra)$} \tcp*[1]{Heuristic algorithm which designs experiments}}
    \KwIn{\ttt{RS($\pra$)} \tcp*[1]{Resampling algorithm for redrawing particles}}

    \KwOut{$\al^{\prime}$ \tcp*[1]{estimate of Hamiltonian parameters}}\;


    Sample $\Np$ times from $\pra \gets \mathcal{P}$ \tcp*[1]{particles}\;
    \For{
        $p \in \mathcal{P}$ 
    }{
        $w_p \gets 1/\Np$ \tcp*[1]{set weights for each particle}
    }\;
    \For{$ e \in  \{1 \rightarrow \Ne\}$ }{ 
        
        $t, \ket{\psi} \gets \Lambda(\pra)$ \tcp*[1]{design an experiment}
        
        \For{
            $p \in \mathcal{P}$ 
        }{
            Retrieve particle $p \gets \al_p$
            
            Prepare $Q$ in $\ket{\psi}$, evolve and measure after $t \gets d$ \tcp*[1]{datum}
            
            $\lvert \bra{d} e^{-iH(\al_p)t} \ket{\psi} \rvert^2 \gets \Pr(d|\al_p; t)$ \tcp*[1]{likelihood}
                        
            $w_p \gets w_p \times \Pr(d|\al_p; t)$ \tcp*[1]{weight update}
        }
        \If{
            $ 1 / \sum_p w_p^2 < \Np/2$
            \tcp*[1]{check whether to resample (are weights too small?)}
        }{
            $\ttt{RS}(\pra) \gets \mathcal{P}$
            \tcp*[1]{Redraw particles via resampling algorithm}
            \For{
                $p \in \mathcal{P}$ 
            }{
                $w_p \gets 1/\Np$ \tcp*[1]{set weights for each particle}
            }
        }
    }
    
    \ttt{mean}($\pra) \gets \vec{\alpha}^{\prime}$\;
    
    return $\al^{\prime}$
     
\end{algorithm}



\begin{algorithm}
    \caption{Bayes Factor calculation}
    \label{alg:bayes_factor}
    \DontPrintSemicolon
    \KwIn{ $Q$ \tcp*[1]{some physically measurable or simulateable quantum system.}}
    \KwIn{ $\hat{H}_j^{\prime}, \hat{H}_k^{\prime}$ \tcp*[1]{Hamiltonian models after training (i.e. $\al_j, \al_k$ already optimised), on which to compare performance.}}
    \KwIn{ $\expset_j, \expset_k$ \tcp*[1]{experiments on which $\hat{H}_j^{\prime}$ and $\hat{H}_k^{\prime}$ were trained during QHL.}}
    
    \KwOut{$B_{jk}$ \tcp*[1]{Bayes factor between two candidate Hamiltonians}}\;
          
    $\expset = \{ \expset_j \cup \expset_k \}$ \tcp*[1]{common experiments for fair comparison}\;
    \For{$\hat{H}_i^{\prime} \in  \{\hat{H}_j^{\prime}, \hat{H}_k^{\prime}$ \} }{
        $\tll_{i} = 0$ \tcp*[1]{total log-likelihood of $\hat{H}_i$} 
        \For{$e \in \expset$}{
            $e \gets t, \ket{\psi}$ \tcp*[1]{assign time and \gls{probe} from experiment control set}\;

            Prepare $Q$ in $\ket{\psi}$, evolve and measure after $t \gets d$ \tcp*[1]{datum}\;

            $ \absval{ \bra{d} e^{-i \hat{H}_i^{\prime} t} \ket{\psi} }^2 \gets \Pr(d | \hat{H}_i, t)$  \tcp*[1]{total \gls{likelihood} for $\hat{H}_i^{\prime}$ on $e$}\;

            $log \left( \Pr(d | \hat{H}_i, t) \right) \gets l_e$ \tcp*[1]{log total \gls{likelihood} for $\hat{H}_i^{\prime}$ on $e$}\;

            $\tll_{i}+ l_e \gets  \tll_{i}$ \tcp*[1]{add $l_e$ to total log total \gls{likelihood} }
        }
    }
    
    $\exp\left( \tll_{j} - \tll_k \right) \gets B_{jk}$  \tcp*[1]{Bayes factor between models}\;
    return $B_{jk}$
\end{algorithm}


